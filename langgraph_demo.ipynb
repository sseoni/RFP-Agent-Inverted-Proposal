{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain\n",
    "%pip install langgraph\n",
    "%pip install langchain-openai\n",
    "%pip install -U langchain-community\n",
    "%pip install -U langchain_huggingface\n",
    "%pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "loader = WebBaseLoader(\"https://huggingface.co/docs/transformers/index\")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "split_documents = text_splitter.split_documents(docs)\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "vectorstore = FAISS.from_documents(documents=split_documents, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"You are an assistant for question-answering tasks.\n",
    "    Use the following pieces of retrieved context to answer the question.\n",
    "    If you don't know the answer, just say that you don't know.\n",
    "    \n",
    "    #Question: \n",
    "    {question} \n",
    "    #Context: \n",
    "    {context} \n",
    "\n",
    "    #Answer:\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    max_tokens=4096,\n",
    "    temperature=1.2,\n",
    ")\n",
    "\n",
    "chat_memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=512,\n",
    "    memory_key=\"history\",\n",
    "    return_messages=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, List, Optional\n",
    "\n",
    "class graphState(TypedDict):\n",
    "    question: str\n",
    "    answer: str\n",
    "    context: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_document(state: graphState) -> graphState:\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    retrieved_docs = retriever.invoke(state[\"question\"])\n",
    "    return graphState(context=format_docs(retrieved_docs))\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "def chatbot(state: graphState) -> graphState:\n",
    "    response = llm.invoke(f\"Context: {state['context']}\\nQuestion: {state['question']}\\nAnswer:\")\n",
    "    return {\"question\": state[\"question\"], \"context\": state[\"context\"], \"answer\": response}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "workflow = StateGraph(graphState)\n",
    "\n",
    "workflow.add_node(\"retrieve\", retrieve_document)\n",
    "workflow.add_node(\"chatbot\", chatbot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.set_entry_point(\"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"chatbot\")\n",
    "workflow.add_edge(\"chatbot\", END)\n",
    "# workflow.add_edge(\"chatbot\", \"relevance_check\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=workflow_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GroundednessCheck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_upstage import UpstageGroundednessCheck\n",
    "\n",
    "upstage_ground_checker = UpstageGroundednessCheck()\n",
    "\n",
    "def relevance_check(state: graphState) -> graphState:\n",
    "\tresponse = upstage_ground_checker.run(\n",
    "\t\t{\"context\": state[\"context\"], \"answer\": state[\"answer\"]}\n",
    "\t)\n",
    "\treturn graphState(\n",
    "\t\trelevance=response,\n",
    "\t\tcontext=state[\"context\"],\n",
    "\t\tanswer=state[\"answer\"],\n",
    "\t\tquestion=state[\"question\"],\n",
    "\t)\n",
    "\t\n",
    "workflow.add_conditional_edges(\n",
    "\t\"relevance_check\", # 추가할 노드 이름\n",
    "\tis_relevant,       # relevance_check 노드에서 나온 결과를 is_relevant 함수에 전달합니다.\n",
    "\t# is_relevant함수는 grounded, notGrounded, notSure 중 하나를 return\n",
    "\t# grounded를 반환하면 END노드로 이동 -> 그래프 실행 종료\n",
    "\t# notGrounded거나 notSure이면 llm_answer노드로 연결\n",
    "\t{\n",
    "\t\t\"grounded\": END,\n",
    "\t\t\"notGrounded\": \"llm_answer\",\n",
    "\t\t\"notSure\": \"llm_answer\"\n",
    "\t},\n",
    ")\n",
    "\n",
    "upstage_ground_checker.run(\n",
    "\t{\n",
    "\t\t\"context\": format_docs(\n",
    "\t\t\tretrieve_document.invoke(\"what is Transformers for?\")\n",
    "\t\t),\n",
    "\t\t\"answer\": \"삼성전자가 개발한 생성AI의 이름은 '빅스비 AI'입니다.\",\n",
    "\t}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"Simple QA Chatbot (Type 'exit' to quit)\")\n",
    "    while True:\n",
    "        user_input = input(\"\\nEnter your question: \")\n",
    "        if user_input.lower() == \"exit\":\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "\n",
    "        inputs = graphState(question=user_input)\n",
    "        config = RunnableConfig(recursion_limit=20, configurable={\"thread_id\": \"huggingface_transformer\"})\n",
    "        output = app.invoke(inputs, config=config)\n",
    "\n",
    "        print(\"\\nAnswer:\", output[\"answer\"])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f1062708a37074d70712b695aadee582e0b0b9f95f45576b5521424137d05fec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
